# Overall architecture of this repository

This document describes the high-level architecture of the review dashboard. If you want to familiarize yourself with the code base, you are just in the right place!

## High-level overview
This repository contains zeo separate, but related pieces of code.

The first half is infrastructure to query and download metadata about all currently open pull requests on *mathlib*, tracking their state over time. A github workflow periodically checks for updates on pull requests and downloads the current data for all pull requests with updates. Currently, this happens about every five minutes.
This data is tracked in the repository: hence, this repository also functions as a cache of information, allowing to only download meta-data for updated PRs (as opposed to download all PRs' data each time the dashboard is re-generated).

The second half contains scripts and a github workflow to generate the mathlib triage and review dashboard, using the data from the previous step. Generating this dashboard proceeds by
- generating a static webpage from this information
- publishing the webpage using github pages
These steps also are run regularly, using a cronjob. (This step happends independently of the other half of the script.) As of November 2024, a workflow run takes about two minutes, and a new job starts every five minutes. All in all, this means the data on the dashboard has a latency of around ten minutes.


## Relevant files
This section talks briefly about various important directories and data structures. Pay attention to the Architecture Invariant sections. They often talk about things which are deliberately absent in the source code.

**Part 1: data gathering infrastructure**
The `data` directory contains all downloaded data for all pull requests to the *mathlib* repository: this ought to contain data for all open pull requests. Data for closed pull requests is gradually being included (but currently, data for many such PRs is still missing).
**Invariant.** All contents in `data` is directly downloaded using the Github API. Any post-processing of data happens in a separate directory.

The `processed_data` directory contains results of data post-processing scripts. Currently, there are three such files, all generated by `process.py`
- `all_pr_data.json` contains certain overview information for every PR with metadata in this repository
- `open_pr_data.json` contains the same information, but only for the subset of currently open PRs
- `assignment_data.json` collects which PRs are assigned to which github user

There are a few text files which hold state, about missing PRs or PRs which might need special handling.
- `missing_prs.txt` lists open PRs for which data is entirely missing
(This could happen, for example, if there is an error in the downloading workflow. In practice, this is rare.)
It also contains comments if downloading a PR's data failed already: if downloading a PR's data failed for three times in a row,
the PR is marked as "stubborn" and listed in `stubborn_prs.txt` instead.
- `stubborn_prs.txt` lists PRs for which we only download reduced metadata. (Some PRs have so many commits, for example, that downloading metadata for all of these would lead to the download timing out.) Less than 1% of all PRs are marked stubborn.
- `closed_prs_to_backfill.txt` lists PRs which were closed a while ago: we want to collect their data, but this is not urgent.
- `redownload.txt` contains PRs for which valid data exists, but that data is outdated: the current data should be downloaded again, but in the mean-time, we can keep the data we have

`check_data_integrity.py` is a script to verify the contents of the downloaded data, and detect broken data. It performs a variety of small tasks
- detect broken json files (and automatically removes them), marking PRs as stubborn if necessary
- detect PRs whose data is surely out of date (and schedules them for re-downloading)
- prunes obsolete entries from `missing_prs.txt` and `closed_prs_to_backfill.txt`

This script depends on quite a bit of file state:
- the entire `data` directory
- the aggregate json files
- all text files about files to (re-)download
- input files `all-open-prs-{1,2}.json` with current data about all open PRs,
  to compare this with the data in the aggregate files

`gather_stats.sh`: TODO document!
`gather_stats.yml`: TODO document!
`gather_stats_single.yml` currently unused; TODO document what it is meant to do!


**Part 2: generating webpages**
`dashboard.sh` (a shell script) is the main entry point:
- it queries github's API for the data above and creates a number of JSON files containing the relevant data
- it then calls `dashboard.py` (with the JSON files passed as explicit arguments) to create a dashboard.
*Currently*, all data has to be regenerated upon every call of the script.

**Architecture invariant.** All network requests happen in this script (TODO and the others!).
`dashboard.py` makes no connections to the network.

`dashboard.py` is where the core logic of creating the dashboard lives. It is a Python script, taking the JSON files from the previous step and the detailed PR information in `data` as input. It writes the HTML code for the dashboard page, and various subpages, to hard-coded HTML files.

**Architecture invariant.** The output of `dashboard.py` only depends on its command line arguments, the contents of the `data` directory and its current time: with both fixed, it is deterministic. In particular, it makes no network requests. All reading of input files is constrained to one method `read_json_files` in the beginning. The output of `dashboard.py` only depends on its command line arguments and its current time: with both fixed, it is deterministic. In particular, it makes no network requests.

`generate_assignment_page.py` is a separate script generating a webpage `assign_reviewers.html` suggesting reviewers to assign to unassigned pull requests. This page is not generated by default, as it is aimed at mathlib maintainers.

`.github/workflows/publish-dashboard.yml` defines the workflow updating the dashboard. It runs the above scripts to generate an up-to-date dashboard, and commits the updated HTML file on the `gh-pages` branch. That branch is deployed by github pages to create the webpage.

`style.css` contains all style rules for the generated webpage

`util.py` contains two functions which are needed in otherwise unrelated scripts

`classify_pr_state.py` contains logic to classify a pull request as ready for review, awaiting action by the author, blocked on another PR, etc. This is used to generate a statistics section on the dashboard. It is called directly by `dashboard.py`.

`test` contains versions of all input files to this script, at some point in time. These can be used for locally testing `dashboard.py`.

**TODO** check if these are all files!


# Cross-cutting concerns

## Limiting github API calls
Each github API call is expensive: github (understandably) adds rate limiting to each call to its API: successive calls can happen at most once per second. This imposes a natural lower bound on the execution time of this script.
In particular, each query for each dashboard takes one second: if easily possible/the data for a particular dashboard can be easily derived (e.g., filtered) from existing data, avoiding an API call and a separate JSON file is preferred.

## Testing
There are several levels at which this project can be tested. Currently, there are no *automated* tests, but an effort is made that the dashboard logic can easily be tested manually.

- `classify_pr_state.py` has unit tests: to run them, use e.g. `nose` (which will pick them up automatically), or uncomment all methods named `test_xxx` and run `python3 classify_pr_state.py`

TODO: there are more generated HTML files now; recommend copying the folder instead...
- changes to just `dashboard.py` can be tested using the JSON files in the `test` directory: run the following from the `test` directory.
`python3 ../dashboard.py all-open-PRs-1.json all-open-PRs-2.json`.
This creates two webpages named `on_the_queue.html` and `index.html`, overwriting any previous files named thus.
You can run this command before and after your changes and compare the resulting files (using `diff` or a similar tool). Because of the overwriting, take care to copy e.g. the old version of the output to a different file before running the tool again.
(The output file needs to be in the top-level directory in order for the styling to work.)

## TODO document
- `data` directory, metadata updating via `gather_stats.sh` (and the other workflow)
- data integrity check (once written)

- additional tools for testing `mypy`, `ruff`, `isort`, (black)
